# awesome-to-be-read
Papers about LLMs that I intend to read

## Inference Acceleration 

[PQCache: Product Quantization-based KVCache for Long Context LLM Inference](https://arxiv.org/abs/2407.12820): Jul 24

[Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/abs/2410.04466) Oct 24 :fire:

## Retrieval
[Rethinking the Role of Token Retrieval in Multi-Vector Retrieval](https://openreview.net/forum?id=ZQzm0Z47jz) Sep 23

[Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118) Dec 21

## Reranking
[Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models](https://openreview.net/forum?id=yvqWdJqYN1): Jun 24

## MS MARCO
[The tale of two MS MARCO - and their unfair comparisons](https://arxiv.org/pdf/2304.12904) Apr 23

## LLM Optimization

### Memory Optimization 
[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054) Oct 19 - Read till the end of deep dive Zero-DP.

### Model Parallelism

### Data Parallelism
