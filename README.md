# awesome-to-be-read
Papers about LLMs that I intend to read

## Inference Acceleration 

[PQCache: Product Quantization-based KVCache for Long Context LLM Inference](https://arxiv.org/abs/2407.12820): Jul 24

## Reranking
[Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models](https://openreview.net/forum?id=yvqWdJqYN1): Jun 24
