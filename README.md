# awesome-to-be-read
Papers about LLMs that I intend to read

## Inference Acceleration 

[PQCache: Product Quantization-based KVCache for Long Context LLM Inference](https://arxiv.org/abs/2407.12820)
